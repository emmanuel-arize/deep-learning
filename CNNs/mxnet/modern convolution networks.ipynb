{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c974bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet\n",
    "from mxnet import gluon,npx,np,autograd\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a1377",
   "metadata": {},
   "source": [
    "## LENET NETWORK\n",
    "<img src='../images/lenet.jpg'>\n",
    "<img src='../images/lenet.svg'>\n",
    "\n",
    " (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 252-3)\n",
    "  \n",
    "\n",
    "To read more on LENET visit :\n",
    "\n",
    "<a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf'>GradientBased Learning Applied to Document Recognition</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fd9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet=nn.Sequential()\n",
    "lenet.add(nn.Conv2D(channels=6,padding=2,kernel_size=5,activation='sigmoid'),\n",
    "          nn.AvgPool2D(pool_size=2,strides=2),\n",
    "          nn.Conv2D(channels=16,kernel_size=5,activation='sigmoid'),\n",
    "          nn.AvgPool2D(pool_size=2,strides=2),\n",
    "         nn.Dense(120, activation='sigmoid'),\n",
    "         nn.Dense(84, activation='sigmoid'),\n",
    "         nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad8fd6",
   "metadata": {},
   "source": [
    "As compared to the original network, we took the liberty of replacing the Gaussian activation in\n",
    "the last layer by a regular dense layer, which tends to be significantly more convenient to train.\n",
    "Other than that, this network matches the historical definition of LeNet5.\n",
    "Next, let us take a look of an example. As shown in Fig. 6.6.2, we feed a single-channel example\n",
    "of size 28 × 28 into the network and perform a forward computation layer by layer printing the\n",
    "output shape at each layer to make sure we understand what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b50c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5 output shape:\t (1, 6, 28, 28)\n",
      "pool3 output shape:\t (1, 6, 14, 14)\n",
      "conv6 output shape:\t (1, 16, 10, 10)\n",
      "pool4 output shape:\t (1, 16, 5, 5)\n",
      "dense3 output shape:\t (1, 120)\n",
      "dense4 output shape:\t (1, 84)\n",
      "dense5 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 28, 28))\n",
    "lenet.initialize()\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b8fab",
   "metadata": {},
   "source": [
    "## ALEXNET\n",
    "<img src='../images/alexnet.jpg'>\n",
    " (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 261)\n",
    "  \n",
    "To read more on Alexnet visit :\n",
    "\n",
    "<a href='https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf'>ImageNet Classification with Deep Convolutional\n",
    "Neural Networks\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2cf899",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet=nn.Sequential()\n",
    "alexnet.add(nn.Conv2D(channels=96,kernel_size=11,strides=4,activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3,strides=2),\n",
    "            nn.Conv2D(channels=256,kernel_size=5,padding=2,activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3,strides=2),\n",
    "            nn.Conv2D(channels=384,kernel_size=3,padding=1,activation='relu'),\n",
    "            nn.Conv2D(channels=384,kernel_size=3,padding=1,activation='relu'),\n",
    "            nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3, strides=2),\n",
    "            # Here, the number of outputs of the fully connected layer is several\n",
    "            # times larger than that in LeNet. Use the dropout layer to mitigate\n",
    "            # overfitting\n",
    "            nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "            nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "            # Output layer. Since we are using Fashion-MNIST, the number of\n",
    "            # classes is 10, instead of 1000 as in the paper\n",
    "            nn.Dense(10)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6967e7",
   "metadata": {},
   "source": [
    "We construct a single-channel data instance with both height and width of 224 to observe the output shape of each layer. It matches our diagram above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1936020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv7 output shape:\t (1, 96, 54, 54)\n",
      "pool5 output shape:\t (1, 96, 26, 26)\n",
      "conv8 output shape:\t (1, 256, 26, 26)\n",
      "pool6 output shape:\t (1, 256, 12, 12)\n",
      "conv9 output shape:\t (1, 384, 12, 12)\n",
      "conv10 output shape:\t (1, 384, 12, 12)\n",
      "conv11 output shape:\t (1, 384, 12, 12)\n",
      "pool7 output shape:\t (1, 384, 5, 5)\n",
      "dense6 output shape:\t (1, 4096)\n",
      "dropout2 output shape:\t (1, 4096)\n",
      "dense7 output shape:\t (1, 4096)\n",
      "dropout3 output shape:\t (1, 4096)\n",
      "dense8 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 224, 224))\n",
    "alexnet.initialize()\n",
    "for layer in alexnet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516eb4e6",
   "metadata": {},
   "source": [
    "## VGG Blocks\n",
    "<img src='../images/vvg.jpg'>\n",
    "The function takes two arguments corresponding to the number of convolutional layers num_convs and the number of output channels num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d9f4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size=3,padding=1, activation='relu'))\n",
    "    blk.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea259877",
   "metadata": {},
   "source": [
    "The original VGG network had 5 convolutional blocks, among which the first two have one convolutional layer each and the latter three contain two convolutional layers each. The first block has\n",
    "64 output channels and each subsequent block doubles the number of output channels, until that\n",
    "number reaches 512. Since this network uses 8 convolutional layers and 3 fully-connected layers,\n",
    "it is often called VGG-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9700330",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc53eb0",
   "metadata": {},
   "source": [
    "The following code implements VGG-11. This is a simple matter of executing a for loop over\n",
    "conv_arch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7091881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net=nn.Sequential()\n",
    "    for (num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs,num_channels))\n",
    "    net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),nn.Dense(4096, activation='relu'),\n",
    "            nn.Dropout(0.5),nn.Dense(10))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e846a",
   "metadata": {},
   "source": [
    "Next, we will construct a single-channel data example with a height and width of 224 to observe\n",
    "the output shape of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e79f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential4 output shape:\t (1, 64, 112, 112)\n",
      "sequential5 output shape:\t (1, 128, 56, 56)\n",
      "sequential6 output shape:\t (1, 256, 28, 28)\n",
      "sequential7 output shape:\t (1, 512, 14, 14)\n",
      "sequential8 output shape:\t (1, 512, 7, 7)\n",
      "dense9 output shape:\t (1, 4096)\n",
      "dropout4 output shape:\t (1, 4096)\n",
      "dense10 output shape:\t (1, 4096)\n",
      "dropout5 output shape:\t (1, 4096)\n",
      "dense11 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "vgg11 = vgg(conv_arch)\n",
    "vgg11.initialize()\n",
    "X = np.random.uniform(size=(1, 1, 224, 224))\n",
    "for blk in vgg11:\n",
    "    X = blk(X)\n",
    "    print(blk.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ed8bd",
   "metadata": {},
   "source": [
    "##  Network in Network (NiN) BLOCK\n",
    "<img src='../images/nin.jpg'/>\n",
    "The NiN block consists of one convolutional layer followed by two 1 × 1 convolutional layers that\n",
    "act as per-pixel fully-connected layers with ReLU activations. The convolution width of the first\n",
    "layer is typically set by the user. The subsequent widths are fixed to 1 × 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7dfd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35230461",
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = nn.Sequential()\n",
    "nin.add(nin_block(96, kernel_size=11, strides=4, padding=0),nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(256, kernel_size=5, strides=1, padding=2),nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(384, kernel_size=3, strides=1, padding=1),nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nn.Dropout(0.5),\n",
    "        # There are 10 label classes\n",
    "        nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "         # The global average pooling layer automatically sets the window shape\n",
    "          # to the height and width of the input\n",
    "           nn.GlobalAvgPool2D(),\n",
    "           # Transform the four-dimensional output into two-dimensional output\n",
    "           # with a shape of (batch size, 10)\n",
    "        nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4315c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential10 output shape:\t (1, 96, 54, 54)\n",
      "pool13 output shape:\t (1, 96, 26, 26)\n",
      "sequential11 output shape:\t (1, 256, 26, 26)\n",
      "pool14 output shape:\t (1, 256, 12, 12)\n",
      "sequential12 output shape:\t (1, 384, 12, 12)\n",
      "pool15 output shape:\t (1, 384, 5, 5)\n",
      "dropout6 output shape:\t (1, 384, 5, 5)\n",
      "sequential13 output shape:\t (1, 10, 5, 5)\n",
      "pool16 output shape:\t (1, 10, 1, 1)\n",
      "flatten0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 224, 224))\n",
    "nin.initialize()\n",
    "for layer in nin:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a02168",
   "metadata": {},
   "source": [
    "## INCEPTION\n",
    "<img src=\"../images/inception.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35a79d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(nn.Block):\n",
    "    def __init__(self,c1,c2,c3,c4,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Path 1 is a single 1 x 1 convolutional layer\n",
    "        self.p1_1=nn.Conv2D(c1,kernel_size=1,activation='relu')\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\n",
    "        # convolutional layer\n",
    "        self.p2_1=nn.Conv2D(c2[0],kernel_size=1,activation='relu')\n",
    "        self.p2_2=nn.Conv2D(c2[1],kernel_size=3,padding=1,activation='relu')\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 5 x 5\n",
    "        # convolutional layer\n",
    "        self.p3_1=nn.Conv2D(c3[0],kernel_size=1,activation='relu')\n",
    "        self.p3_2=nn.Conv2D(c3[1],kernel_size=5,padding=2,activation='relu')\n",
    "        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\n",
    "        # convolutional layer\n",
    "        self.p4_1=nn.MaxPool2D(pool_size=3,padding=1,strides=1)\n",
    "        self.p4_2=nn.Conv2D(c4,kernel_size=1,activation='relu')\n",
    "    def forward(self,x):\n",
    "        p1=self.p1_1(x)\n",
    "        p2=self.p2_2(self.p2_1(x))\n",
    "        p3=self.p3_2(self.p3_1(x))\n",
    "        p4=self.p4_2(self.p4_1(x))\n",
    "        return np.concatenate((p1,p2,p3,p4),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573cf81",
   "metadata": {},
   "source": [
    "GoogLeNet uses a stack of a total of 9 inception blocks and global average pooling to generate its estimates. Maximum pooling between inception blocks reduced the\n",
    "dimensionality. The first part is identical to AlexNet and LeNet, the stack of blocks is inherited\n",
    "from VGG and the global average pooling avoids a stack of fully-connected layers at the end. The\n",
    "architecture is depicted below\n",
    "<img src=\"../images/inception1.jpg\" />\n",
    "<img src=\"../images/inception2.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26c4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception=nn.Sequential()\n",
    "inception.add(nn.Conv2D(64,kernel_size=7,strides=2,padding=1,activation='relu'),\n",
    "              nn.MaxPool2D(pool_size=3,padding=1,strides=2),\n",
    "              \n",
    "              nn.Conv2D(64,kernel_size=1,activation='relu'),\n",
    "              nn.Conv2D(192,kernel_size=3,padding=1,activation='relu'),\n",
    "              nn.MaxPool2D(pool_size=3,padding=1,strides=2),\n",
    "              # inception(3a)\n",
    "              Inception_block(c1=64,c2=(96,128),c3=(16,32),c4=32),\n",
    "              # inception(3b)\n",
    "              Inception_block(c1=128,c2=(128,192),c3=(32,96),c4=64),\n",
    "              nn.MaxPool2D(pool_size=3,strides=2,padding=1),\n",
    "              # inception(4a)\n",
    "              Inception_block(c1=192,c2=(96,208),c3=(16,48),c4=64),\n",
    "              # inception(4b)\n",
    "              Inception_block(c1=160,c2=(112,224),c3=(24,64),c4=64),\n",
    "              # inception(4c)\n",
    "              Inception_block(c1=128,c2=(128,256),c3=(24,64),c4=64),\n",
    "              # inception(4d)\n",
    "              Inception_block(112,(144,288),(32,64),64),\n",
    "              # inception(4e)\n",
    "              Inception_block(256,(160,320),(32,128),128),\n",
    "              nn.MaxPool2D(pool_size=3,strides=2,padding=1),\n",
    "              # inception(5a)\n",
    "              Inception_block(256, (160, 320), (32, 128), 128),\n",
    "              # inception(5b)\n",
    "              Inception_block(384, (192, 384), (48, 128), 128),\n",
    "              nn.GlobalAvgPool2D(),\n",
    "              nn.Dense(10)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e9ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv32 output shape:\t (1, 64, 46, 46)\n",
      "pool17 output shape:\t (1, 64, 23, 23)\n",
      "conv33 output shape:\t (1, 64, 23, 23)\n",
      "conv34 output shape:\t (1, 192, 23, 23)\n",
      "pool18 output shape:\t (1, 192, 12, 12)\n",
      "inception_block0 output shape:\t (1, 256, 12, 12)\n",
      "inception_block1 output shape:\t (1, 480, 12, 12)\n",
      "pool21 output shape:\t (1, 480, 6, 6)\n",
      "inception_block2 output shape:\t (1, 512, 6, 6)\n",
      "inception_block3 output shape:\t (1, 512, 6, 6)\n",
      "inception_block4 output shape:\t (1, 512, 6, 6)\n",
      "inception_block5 output shape:\t (1, 528, 6, 6)\n",
      "inception_block6 output shape:\t (1, 832, 6, 6)\n",
      "pool27 output shape:\t (1, 832, 3, 3)\n",
      "inception_block7 output shape:\t (1, 832, 3, 3)\n",
      "inception_block8 output shape:\t (1, 1024, 3, 3)\n",
      "pool30 output shape:\t (1, 1024, 1, 1)\n",
      "dense12 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 96, 96))\n",
    "inception.initialize()\n",
    "for layer in inception:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d0ff8",
   "metadata": {},
   "source": [
    "# RESNET\n",
    "\n",
    "<img src=\"../images/resnet.jpg\"  width='1000px'>\n",
    "Source:  <a href='https://arxiv.org/pdf/1512.03385.pdf'>Deep Residual Learning for Image Recognition<a/>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52811f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Block):\n",
    "    def __init__(self,num_channels,strides=1,downsample=False,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cov1=nn.Conv2D(num_channels,kernel_size=3,strides=strides,padding=1)\n",
    "        self.cov2=nn.Conv2D(num_channels,kernel_size=3,padding=1)\n",
    "        if downsample:\n",
    "            self.downsample=nn.Conv2D(num_channels,kernel_size=1,strides=strides)\n",
    "        else:\n",
    "            self.downsample=None\n",
    "        self.bn1=nn.BatchNorm()\n",
    "        self.bn2=nn.BatchNorm()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h_x=npx.relu(self.bn1(self.cov1(x)))\n",
    "        h_x=self.bn2(self.cov2(h_x))\n",
    "        if self.downsample:\n",
    "            x=self.downsample(x)\n",
    "        return npx.relu(h_x+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89ab1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(num_channels,num_residuals,first_block=False):\n",
    "    blk=nn.Sequential()\n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not first_block:\n",
    "            blk.add(Residual(num_channels,downsample=True,strides=2))\n",
    "        else:\n",
    "            blk.add(Residual(num_channels))\n",
    "    return blk\n",
    "\n",
    "resnet34=nn.Sequential()\n",
    "resnet34.add(  nn.Conv2D(64,kernel_size=7,strides=2,padding=3),\n",
    "             nn.BatchNorm(),\n",
    "             nn.Activation('relu'),\n",
    "             nn.MaxPool2D(pool_size=3,strides=2,padding=1),\n",
    "             resnet_block(num_channels=64,num_residuals=3,first_block=True),\n",
    "             resnet_block(128,4),\n",
    "             resnet_block(256,6),\n",
    "             resnet_block(512,3),\n",
    "             nn.GlobalAvgPool2D(),\n",
    "             nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01222386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv89 output shape:\t (1, 64, 14, 14)\n",
      "batchnorm0 output shape:\t (1, 64, 14, 14)\n",
      "relu0 output shape:\t (1, 64, 14, 14)\n",
      "pool31 output shape:\t (1, 64, 7, 7)\n",
      "sequential16 output shape:\t (1, 64, 7, 7)\n",
      "sequential17 output shape:\t (1, 128, 4, 4)\n",
      "sequential18 output shape:\t (1, 256, 2, 2)\n",
      "sequential19 output shape:\t (1, 512, 1, 1)\n",
      "pool32 output shape:\t (1, 512, 1, 1)\n",
      "dense13 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X=np.random.uniform(size=(1,1,28,28))\n",
    "resnet34.initialize()\n",
    "for layer in resnet34:\n",
    "    X=layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6df69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63c623c0",
   "metadata": {},
   "source": [
    "<p>References </p>\n",
    "\n",
    "<a href='https://arxiv.org/pdf/1512.03385.pdf'>Deep Residual Learning for Image Recognition<a/>\n",
    "    \n",
    "<a href='https://arxiv.org/pdf/1603.05027.pdf'>Identity Mappings in Deep Residual Networks</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a2785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Word-embeddings\" data-toc-modified-id=\"Word-embeddings-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Word embeddings</a></span></li></ul></li></ul></li><li><span><a href=\"#The-Embedding-layer\" data-toc-modified-id=\"The-Embedding-layer-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The Embedding layer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform machine learning on text documents, the raw (text) data cannot be fed directly to algorithm as these algorithms expect numerical feature vectors so instead we need to turn the text content into numerical feature vectors.\n",
    "\n",
    "From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html):\n",
    "<b>\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "</b>\n",
    "So Vectorizing text is the process of transforming text into numeric tensors. \n",
    "\n",
    "Vectorization can be done in multiple ways:\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "N-grams are overlapping groups of multiple consecutive words or characters\n",
    "\n",
    "<b>Tokenization: </b> the segementation of text into words or characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Text-vectorization processes consist of applying some tokenization scheme to the text then associating numeric vectors with the generated tokens.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Unlike the one-hot encoding, word embeddings are learned from data.\n",
    "\n",
    "\n",
    "<P>Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). </P>\n",
    "From <a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">Tensorflow documentation-Word embeddings</a>\n",
    "\n",
    "#### Word embeddings\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "<img src=\"images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Embedding layer\n",
    "From <a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">Tensorflow documentation-Word embeddings</a>\n",
    "> Keras makes it easy to use word embeddings. Take a look at the Embedding layer.\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
    "\n",
    "> When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on). If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir=os.path.dirname('./data/aclImdb/')\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train=True):\n",
    "    labels=[]\n",
    "    texts=[]\n",
    "    if is_train==True:\n",
    "        train_dir = os.path.join(data_dir, 'train')\n",
    "        for label in ['neg','pos']:\n",
    "            dir_name=os.path.join(train_dir,label)\n",
    "            for fname in os.listdir(dir_name):\n",
    "                read_file=open(os.path.join(dir_name,fname),'r',encoding='utf-8').read().lower().replace('\\n','')\n",
    "                texts.append(read_file)\n",
    "                labels.append(1 if label=='pos' else 0)  \n",
    "    else:\n",
    "        train_dir = os.path.join(data_dir, 'test')\n",
    "        for label in ['neg','pos']:\n",
    "            dir_name=os.path.join(train_dir,label)\n",
    "            for fname in os.listdir(dir_name):\n",
    "                read_file=open(os.path.join(dir_name,fname),'r',encoding='utf-8').read().lower().replace('\\n','')\n",
    "                texts.append(read_file)\n",
    "                labels.append(1 if label=='pos' else 0)\n",
    "    return texts,labels\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts,labels =load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the 15000 most common words\n",
    "max_features=15000\n",
    "embedding_dim=16\n",
    "tokenizer=Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "def preprocess_text(text):\n",
    "    sequence=tokenizer.texts_to_sequences(text)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=preprocess_text(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 4, 3, 129, 34, 44, 7576, 1414, 15, 3, 4252, 514, 43, 16, 3, 633, 133, 12, 6, 3, 1301, 459, 4, 1751, 209, 3, 10785, 7693, 308, 6, 676, 80, 32, 2137, 1110, 3008, 31, 1, 929, 4, 42, 5120, 469, 9, 2665, 1751, 1, 223, 55, 16, 54, 828, 1318, 847, 228, 9, 40, 96, 122, 1484, 57, 145, 36, 1, 996, 141, 27, 676, 122, 1, 13886, 411, 59, 94, 2278, 303, 772, 5, 3, 837, 11037, 20, 3, 1755, 646, 42, 125, 71, 22, 235, 101, 16, 46, 49, 624, 31, 702, 84, 702, 378, 3493, 12997, 2, 8422, 67, 27, 107, 3348]\n"
     ]
    }
   ],
   "source": [
    "print(sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_from_seq(seq):\n",
    "    words=[tokenizer.index_word.get(i) for i in seq]\n",
    "    return ' '.join(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy of it's singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level it's better than you might think with some good cinematography by future great future stars sally kirkland and forrest can be seen briefly\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=sent_from_seq(sequence[0])\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> number of tokens in all the sequences.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens=[len(tokens) for tokens in sequence]\n",
    "max_tokens=int(np.mean(num_tokens))\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence_pad=pad_sequences(sequence,maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 228, 16)           240000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3648)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 34)                124066    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 364,101\n",
      "Trainable params: 364,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=max_features,output_dim=embedding_dim,input_length=max_tokens))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(34,activation='relu'))\n",
    "model.add(Dropout(0.07))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 228), (25000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence_pad.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 15ms/step - loss: 0.5898 - acc: 0.6764 - val_loss: 0.5233 - val_acc: 0.7586\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.2185 - acc: 0.9151 - val_loss: 0.7345 - val_acc: 0.6914\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1252 - acc: 0.9559 - val_loss: 0.6441 - val_acc: 0.7616\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0604 - acc: 0.9838 - val_loss: 0.7437 - val_acc: 0.7598\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0249 - acc: 0.9942 - val_loss: 0.7609 - val_acc: 0.7926\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0090 - acc: 0.9983 - val_loss: 1.0975 - val_acc: 0.7420\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.2670 - val_acc: 0.7472\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 1.6201 - val_acc: 0.7180\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 2.6134e-04 - acc: 0.9999 - val_loss: 1.6701 - val_acc: 0.7374\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 3.7569e-05 - acc: 1.0000 - val_loss: 1.4313 - val_acc: 0.7924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_sequence_pad, labels,epochs=10,batch_size=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING THE TRAINED MODEL ON THE TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts,y_test=load_data(is_train=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess__test_text(text):\n",
    "    sequence=tokenizer.texts_to_sequences(text)\n",
    "    test_sequence_pad=pad_sequences(sequence,maxlen=max_tokens)\n",
    "    return test_sequence_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   32,  531,    8],\n",
       "       [   4,  135,    1, ...,  176,  467,  155],\n",
       "       [   0,    0,    0, ...,    8,    1,  174],\n",
       "       ...,\n",
       "       [ 144,  320,    4, ...,   34,  314,   38],\n",
       "       [   0,    0,    0, ...,   28, 1156, 5894],\n",
       "       [   0,    0,    0, ...,   58,  104, 3194]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text=preprocess__test_text(test_texts)\n",
    "x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 228)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step - loss: 0.9427 - acc: 0.8475\n"
     ]
    }
   ],
   "source": [
    "loss,accu=model.evaluate(x_text,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.75%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Not a good movie!\"\n",
    "text2=\"The movie was great!\"\n",
    "text3=\"The movie was terrible...\"\n",
    "text4=\"This is a confused movie.\"\n",
    "text5 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text6 = \"This movie really sucks! Can I get my money back please?\"\n",
    "text7='the animation and graphics were very good but The movie was too bad so am confused whether to recommend this movie or not.'\n",
    "textss = [text1, text2, text3, text4, text5,text6,text7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(text):\n",
    "    seq=preprocess__test_text(textss)\n",
    "    pred=model.predict(seq)\n",
    "    pred=np.array(pred)\n",
    "    result=[text+': ==== >positive review' if i>0.5 else text + ': ===>negative review'  for text,i in zip(textss,pred)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not a good movie!: ===>negative review',\n",
       " 'The movie was great!: ===>negative review',\n",
       " 'The movie was terrible...: ===>negative review',\n",
       " 'This is a confused movie.: ===>negative review',\n",
       " 'This movie is fantastic! I really like it because it is so good!: ==== >positive review',\n",
       " 'This movie really sucks! Can I get my money back please?: ===>negative review',\n",
       " 'the animation and graphics were very good but The movie was too bad so am confused whether to recommend this movie or not.: ===>negative review']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(textss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

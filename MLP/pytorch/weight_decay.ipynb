{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-training-error-is-the-error-of-our-model-as-calculated-on-the-training-dataset\" data-toc-modified-id=\"The-training-error-is-the-error-of-our-model-as-calculated-on-the-training-dataset-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>The training error is the error of our model as calculated on the training dataset</a></span></li><li><span><a href=\"#The-generalization-error-is-the-expected-value-of-the-error-on-a-test-or-new-data-points-drawn-from-the-same-underlying-data-distribution-as-our-original-sample\" data-toc-modified-id=\"The-generalization-error-is-the-expected-value-of-the-error-on-a-test-or-new-data-points-drawn-from-the-same-underlying-data-distribution-as-our-original-sample-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>The generalization error is the expected value of the error on a test or new data points drawn from the same underlying data distribution as our original sample</a></span></li></ul></li><li><span><a href=\"#Regularization-are-techniques-used-to-combat-overfitting--and-this-reduces-the-test-error-or-generalization-erro\" data-toc-modified-id=\"Regularization-are-techniques-used-to-combat-overfitting--and-this-reduces-the-test-error-or-generalization-erro-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Regularization are techniques used to combat overfitting  and this reduces the test error or generalization erro</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection, Underfitting and Overfitting\n",
    "\n",
    "-------\n",
    "\n",
    "The goal of machine learning is find a learning algorithm (algorithm that is able to learn from data) or simply model, train a model by adjusting the model parameters to get the best possible performance, both on the training (with minimum training error) and the test dat or new inputs (the trained model must be able to generalized well with minimum generalization error or test error) but the challenge in machine learning is how well does the trained model perform not just on the training data, but also on new unseen inputs (test inputs).This is a fundamental problem in machine learning between <b> optimization (the process of adjusting a model parameters to give the best possible performance on the training data) and generalization (how well does the trained model performs on newly unseen data) because a trained model can perform well well on the training dataset but performs poorly on newly unseen data points.</b>\n",
    "\n",
    "\n",
    "# NOTE\n",
    "------\n",
    "<h3 style='color:blue'>The training error is the error of our model as calculated on the training dataset</h3>\n",
    "<h3 style='color:blue'>The generalization error is the expected value of the error on a test or new data points drawn from the same underlying data distribution as our original sample</h3>\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "```\n",
    "The factors determining how well a machine learning algorithm will perform are its ability to:\n",
    "1. Make the training error small.\n",
    "2. Make the gap between training and test error small.\n",
    "These two factors correspond to the two central challenges in machine learning: underfitting and overfitting.\n",
    "\n",
    "(source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 111) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overftting\n",
    "\n",
    "-------\n",
    "When the complexity of the model is too high (highly flexible models) as compared to the underlying distribution of the data the model is trying to learn from, it tends to learn the noise present in data and is called overfitting. An Overfitted models has it training error much lower than validation error. <b>An overfitted model fails to Generalize well and has high Variance and Low Bias and the techniques used to combat overfitting are called regularization</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting\n",
    "----\n",
    "Underfitting occurs when the model can neither obtain sufficiently low error value on the training set nor generalize to new data and has low Variance and high Bias. Underfitted models are not able to reduce the training error. W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "----\n",
    "\n",
    "## Regularization are techniques used to combat overfitting  and this reduces the test error or generalization erro\n",
    "\n",
    "```\n",
    " Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error \n",
    " but not its training error\n",
    " \n",
    " (source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 120)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  WEIGHT  REGULARIZATION\n",
    "<img src='images/we.jpg'>\n",
    "(source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 120)\n",
    "<img src='images/weight.jpg'>\n",
    "(source: From the book, Deep Learning with python by Fran√ßois Chollet, page 107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Weight decay is also known as L2 regularization or ridge regression or Tikhonov regularization\n",
    "\n",
    "<b>L2 regularization is also called weight decay in the context of neural networks</b> prevent the weights from growing too large unless it is really necessary. It can be realized by\n",
    "adding a term to the cost OR objective function that penalizes large weights and is defined as\n",
    "\n",
    "$$\\tilde{\\ell}(w)=\\ell(w) + \\frac{\\lambda}{2}w^{2} $$\n",
    "\n",
    "where $ \\tilde{\\ell}$ is the regularized cost fucbtion $\\ell_{0}$ is an error measure (usually the sum of squared errors) and $\\lambda$ is a hyperparameter chosen ahead of time that controls how weights are penalized. (weights the relative contribution of the norm penalty term $w^{2} $  relative to the standard objective function $\\ell$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "with the corresponding parameter gradient\n",
    "$$\\bigtriangledown \\tilde{\\ell}_{w}(w)=\\bigtriangledown \\ell_{w}(w) + \\lambda w $$\n",
    "\n",
    "The new updated weight after an iteration can be expressed as\n",
    "$$w=w-\\eta \\bigtriangledown\\tilde{\\ell}_{w}(w)=w-\\eta(\\lambda w +\\bigtriangledown \\ell_{w}(w)) $$\n",
    "\n",
    "$$ w=(1- \\eta\\lambda) w -\\eta \\bigtriangledown \\ell_{w}(w)) $$\n",
    "where $\\eta$ is the learning rate\n",
    "\n",
    "\n",
    "The addition of the weight decay term has modified the learning rule of the weight vector by a constant factor on each step just before updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression, the objective function, sum of squared errors is defined as\n",
    "$$e=(Xw-y)^{T}(Xw-y)$$\n",
    "\n",
    "When L2 regularization is added, the objective function changes to\n",
    "$$e=(Xw-y)^{T}(Xw-y)+ \\frac{\\lambda}{2}w^{2}$$\n",
    "\n",
    "and this the solution $w$ from\n",
    "$$ w=(XX^{T})^{-1}X^{T}y $$\n",
    "\n",
    "$$ To$$\n",
    "\n",
    "$$ w=(XX^{T}   + \\lambda  I )^{-1}X^{T}y $$\n",
    "\n",
    "Where the diagonal entries of this matrix $ \\lambda  I $ correspond to the variance of each input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/lp.jpg'>\n",
    " (source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 155-156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the effects of weight regularization \n",
    "<a href='https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1947&rep=rep1&type=pdf'>A Simple Weight Decay Can Improve Generalization by Anders Krogh and John A. Hertz</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T22:57:55.990446Z",
     "start_time": "2020-09-07T22:57:55.947447Z"
    }
   },
   "source": [
    "# High-Dimensional Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.436524Z",
     "start_time": "2020-09-16T21:22:03.753449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x168844a2bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/highp.jpg'>\n",
    "(source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.451895Z",
     "start_time": "2020-09-16T21:22:06.442523Z"
    }
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X =torch.normal(0,0.01,(num_examples, w.shape[0]))\n",
    "    y=X@w+b + torch.zeros(((X@w+b).shape)).normal_(mean=0,std=0.01)\n",
    "    return X, y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.468896Z",
     "start_time": "2020-09-16T21:22:06.462895Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "lr=0.01\n",
    "lambd=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.484244Z",
     "start_time": "2020-09-16T21:22:06.474895Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train, n_test, num_inputs= 20, 100, 200\n",
    "true_w, true_b=torch.zeros((num_inputs,1))*0.01,0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.508248Z",
     "start_time": "2020-09-16T21:22:06.491244Z"
    }
   },
   "outputs": [],
   "source": [
    "features, labels= synthetic_data(true_w, true_b, n_train)\n",
    "x_test,y_test = synthetic_data(true_w, true_b, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.527247Z",
     "start_time": "2020-09-16T21:22:06.515244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0458],\n",
       "        [0.0567],\n",
       "        [0.0482]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.542248Z",
     "start_time": "2020-09-16T21:22:06.534244Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iter(features,labels,batch_size):\n",
    "    dataset=TensorDataset(*(features,labels))\n",
    "    data_loader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.560244Z",
     "start_time": "2020-09-16T21:22:06.550245Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter=data_iter(features,labels,batch_size=batch_size)\n",
    "test_iter=data_iter(x_test,y_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.597245Z",
     "start_time": "2020-09-16T21:22:06.568245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 200])\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.0441],\n",
      "        [0.0482],\n",
      "        [0.0567],\n",
      "        [0.0618],\n",
      "        [0.0569]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_iter:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.614243Z",
     "start_time": "2020-09-16T21:22:06.603245Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    w_1=torch.normal(0,0.01,(num_inputs,5))\n",
    "    b_1= torch.zeros(5)\n",
    "    w_2=torch.normal(0,0.01,(5,1))\n",
    "    b_2= torch.zeros(1)\n",
    "    w_1.requires_grad_(True)\n",
    "    b_1.requires_grad_(True)\n",
    "    w_2.requires_grad_(True)\n",
    "    b_2.requires_grad_(True)\n",
    "    return [w_1,w_2,b_1,b_2]\n",
    "w_1,w_2, b_1,b_2 = init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T22:43:38.897158Z",
     "start_time": "2020-09-11T22:43:38.888156Z"
    }
   },
   "source": [
    "# L2 REGULARIZATION\n",
    "$$\\tilde{\\ell}(w)=\\ell(w) + \\frac{\\lambda}{2}w^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.635245Z",
     "start_time": "2020-09-16T21:22:06.626243Z"
    }
   },
   "outputs": [],
   "source": [
    "def l2_regularizer(y_hat, y,w_1,w_2,lambd=lambd):\n",
    "    square_error=(y_hat - y.reshape(y_hat.shape)) ** 2 \n",
    "    l2_penalty=(lambd*((w_1**2).sum()+(w_2**2).sum()))/2 \n",
    "    return square_error + l2_penalty  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T15:37:32.884493Z",
     "start_time": "2020-09-12T15:37:32.875489Z"
    }
   },
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.651245Z",
     "start_time": "2020-09-16T21:22:06.642242Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    a=torch.zeros_like(x)\n",
    "    return torch.max(a,x)\n",
    "def Linear(X,w_1,w_2,b_1,b_2):\n",
    "    l1=torch.matmul(X,w_1)+b_1\n",
    "    h1=relu(l1)\n",
    "    output=torch.matmul(h1,w_2)+b_2\n",
    "    return output\n",
    "net=Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:06.667245Z",
     "start_time": "2020-09-16T21:22:06.658247Z"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param.data.sub_(lr*param.grad/batch_size)\n",
    "        param.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:07.442742Z",
     "start_time": "2020-09-16T21:22:06.673245Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 6, training loss :0.002325 testing loss :0.002162\n",
      "epoch : 11, training loss :0.001938 testing loss :0.001790\n",
      "epoch : 16, training loss :0.001603 testing loss :0.001470\n",
      "epoch : 21, training loss :0.001310 testing loss :0.001191\n",
      "epoch : 26, training loss :0.001087 testing loss :0.000981\n",
      "epoch : 31, training loss :0.000913 testing loss :0.000817\n",
      "epoch : 36, training loss :0.000767 testing loss :0.000681\n",
      "epoch : 41, training loss :0.000652 testing loss :0.000575\n",
      "epoch : 46, training loss :0.000555 testing loss :0.000485\n",
      "epoch : 51, training loss :0.000479 testing loss :0.000416\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50# Number of iterations\n",
    "for epoch in range(num_epochs):\n",
    "    epoch+=1\n",
    "    for X,y in train_iter:\n",
    "        pred=net(X,w_1,w_2,b_1,b_2)\n",
    "        loss=l2_regularizer(pred,y,w_1,w_2)\n",
    "    loss.sum().backward()\n",
    "    sgd([w_1,w_2,b_1,b_2],lr=lr,batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_l = l2_regularizer(net(features, w_1,w_2,b_1,b_2), labels,w_1,w_2)\n",
    "        test_l = l2_regularizer(net(x_test, w_1,w_2,b_1,b_2),y_test,w_1,w_2)\n",
    "    if epoch%5==0:\n",
    "        print('epoch : %d, training loss :%f testing loss :%f' % (epoch + 1,\n",
    "                                                                  train_l.mean().numpy(), \n",
    "                                                                  test_l.mean().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCISE IMPLEMENTATION\n",
    "-----\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:07.461738Z",
     "start_time": "2020-09-16T21:22:07.449742Z"
    }
   },
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(200,5)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(5,1)\n",
    "    def forward(self,x):\n",
    "        h1=self.relu(self.linear1(x))\n",
    "        output=self.linear2(h1)\n",
    "        return output\n",
    "nett=LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:07.474741Z",
     "start_time": "2020-09-16T21:22:07.468743Z"
    }
   },
   "outputs": [],
   "source": [
    "opt=torch.optim.SGD(nett.parameters(),lr=lr,weight_decay=lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:07.493743Z",
     "start_time": "2020-09-16T21:22:07.479743Z"
    }
   },
   "outputs": [],
   "source": [
    "mse_err=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.116020Z",
     "start_time": "2020-09-16T21:22:07.500741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 6, training loss :0.031078 testing loss :0.030374\n",
      "epoch : 11, training loss :0.011485 testing loss :0.011062\n",
      "epoch : 16, training loss :0.004261 testing loss :0.004007\n",
      "epoch : 21, training loss :0.001615 testing loss :0.001464\n",
      "epoch : 26, training loss :0.000654 testing loss :0.000564\n",
      "epoch : 31, training loss :0.000304 testing loss :0.000251\n",
      "epoch : 36, training loss :0.000178 testing loss :0.000147\n",
      "epoch : 41, training loss :0.000133 testing loss :0.000115\n",
      "epoch : 46, training loss :0.000116 testing loss :0.000107\n",
      "epoch : 51, training loss :0.000110 testing loss :0.000106\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    epoch +=1\n",
    "    for X,y in train_iter:\n",
    "        prd=nett(X)\n",
    "        #pred=float(pred)\n",
    "        l2_l=mse_err(prd,y)\n",
    "        opt.zero_grad()\n",
    "        l2_l.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        train_l = mse_err(nett(features), labels)\n",
    "        test_l = mse_err(nett(x_test),y_test)\n",
    "    if epoch%5==0:\n",
    "        print('epoch : %d, training loss :%f testing loss :%f' % (epoch + 1, train_l.mean().numpy(), test_l.mean().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.129024Z",
     "start_time": "2020-09-16T21:22:08.122019Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.145020Z",
     "start_time": "2020-09-16T21:22:08.136018Z"
    }
   },
   "outputs": [],
   "source": [
    "model=nn.Sequential(nn.Linear(num_inputs,5),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.159024Z",
     "start_time": "2020-09-16T21:22:08.152020Z"
    }
   },
   "outputs": [],
   "source": [
    "mse_loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.176020Z",
     "start_time": "2020-09-16T21:22:08.167019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of Sequential(\n",
       "  (0): Linear(in_features=200, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.194019Z",
     "start_time": "2020-09-16T21:22:08.182020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Linear(in_features=200, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.208020Z",
     "start_time": "2020-09-16T21:22:08.200019Z"
    }
   },
   "outputs": [],
   "source": [
    "optm=torch.optim.SGD([{'params':model[0].weight,'weight_decay':lambd},{'params':model[0].bias},\n",
    "                      {'params':model[2].weight,'weight_decay':lambd},{'params':model[2].bias},\n",
    "                     ],lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-13T16:58:51.980039Z",
     "start_time": "2020-09-13T16:58:51.964409Z"
    }
   },
   "source": [
    "optm=torch.optim.SGD([{'params':model[0].weight,'weight_decay':lambd},{'params':model[0].bias}],lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.611021Z",
     "start_time": "2020-09-16T21:22:08.232019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocgh 5, loss 0.005293\n",
      "epocgh 10, loss 0.002972\n",
      "epocgh 15, loss 0.000913\n",
      "epocgh 20, loss 0.000554\n",
      "epocgh 25, loss 0.000237\n",
      "epocgh 30, loss 0.000083\n",
      "epocgh 35, loss 0.000171\n",
      "epocgh 40, loss 0.000075\n",
      "epocgh 45, loss 0.000186\n",
      "epocgh 50, loss 0.000023\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    epoch+=1\n",
    "    for x,y in train_iter:\n",
    "        model.train()\n",
    "        pred=model(x)\n",
    "        los=mse_loss(pred,y)\n",
    "        optm.zero_grad()\n",
    "        los.backward()\n",
    "        optm.step()\n",
    "    if epoch%5==0:\n",
    "        print('epocgh %d, loss %f'%(epoch,los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.636023Z",
     "start_time": "2020-09-16T21:22:08.618021Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted=model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.655020Z",
     "start_time": "2020-09-16T21:22:08.646021Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true=y_test.numpy()\n",
    "predicted=predicted.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.668019Z",
     "start_time": "2020-09-16T21:22:08.661021Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted=np.array(predicted)\n",
    "y_true=np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.681021Z",
     "start_time": "2020-09-16T21:22:08.674019Z"
    }
   },
   "outputs": [],
   "source": [
    "d=np.concatenate((y_true,predicted),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.709019Z",
     "start_time": "2020-09-16T21:22:08.688022Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(d,columns=['y_true','predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T21:22:08.777022Z",
     "start_time": "2020-09-16T21:22:08.716021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036797</td>\n",
       "      <td>0.052234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062678</td>\n",
       "      <td>0.049695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049847</td>\n",
       "      <td>0.052587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.062470</td>\n",
       "      <td>0.053157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.066164</td>\n",
       "      <td>0.050192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.051938</td>\n",
       "      <td>0.053144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.043664</td>\n",
       "      <td>0.051151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.059681</td>\n",
       "      <td>0.052491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.049768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050714</td>\n",
       "      <td>0.053505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.051071</td>\n",
       "      <td>0.052837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.043019</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.056118</td>\n",
       "      <td>0.051745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.043349</td>\n",
       "      <td>0.053992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.037939</td>\n",
       "      <td>0.053171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.034412</td>\n",
       "      <td>0.053624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.062066</td>\n",
       "      <td>0.052787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.048475</td>\n",
       "      <td>0.054518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.043670</td>\n",
       "      <td>0.054486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.034280</td>\n",
       "      <td>0.051293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_true  predicted\n",
       "0   0.036797   0.052234\n",
       "1   0.062678   0.049695\n",
       "2   0.049847   0.052587\n",
       "3   0.062470   0.053157\n",
       "4   0.066164   0.050192\n",
       "5   0.051938   0.053144\n",
       "6   0.043664   0.051151\n",
       "7   0.059681   0.052491\n",
       "8   0.052711   0.049768\n",
       "9   0.050714   0.053505\n",
       "10  0.051071   0.052837\n",
       "11  0.043019   0.049200\n",
       "12  0.056118   0.051745\n",
       "13  0.043349   0.053992\n",
       "14  0.037939   0.053171\n",
       "15  0.034412   0.053624\n",
       "16  0.062066   0.052787\n",
       "17  0.048475   0.054518\n",
       "18  0.043670   0.054486\n",
       "19  0.034280   0.051293"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

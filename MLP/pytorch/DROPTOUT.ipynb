{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Test-Time\" data-toc-modified-id=\"Test-Time-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Test Time</a></span></li><li><span><a href=\"#Inverted-Dropout\" data-toc-modified-id=\"Inverted-Dropout-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Inverted Dropout</a></span></li><li><span><a href=\"#Implementing-Inverted-Dropout-from-Scratch\" data-toc-modified-id=\"Implementing-Inverted-Dropout-from-Scratch-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Implementing Inverted Dropout from Scratch</a></span></li><li><span><a href=\"#CONCISE-IMPLEMENTATION\" data-toc-modified-id=\"CONCISE-IMPLEMENTATION-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CONCISE IMPLEMENTATION</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "The term <b>\"dropout\"</b> refers to dropping out units in a neural network. It is a technique for addressing overfitting. It consists of randomly dropping out some fraction of the nodes (setting fraction of the units to zero (injecting noise)) in each layer before calculating subsequent layer during training and has become a standard technique for training neural networks. When dropout is applied, during training its zeros out some fraction of the nodes with probability p in each layer before calculating the subsequent layer and the resulting network can be viewed as a subset of the original network. Because the fraction of the nodes that are drop out are chosen randomly on every pass, the representations in each layer can't depend on the exact values taken by nodes in the previous layer. \n",
    "\n",
    "<b> Dropout rate</b> is the fraction of the nodes in a layer that are zeroed out and itâ€™s usually set between 0 and 1.\n",
    "\n",
    "  \n",
    " ## Test Time\n",
    "\n",
    "<b>Typically at test time we disable dropout.</b> Given a trained model and a new example, we do not drop out any nodes and thus do not need to normalize. \n",
    "In traditional dropout the weights of the network at test time are scaled versions of the trained weights. If a unit is retained with <b>probability q=1-p</b> during training,S at test time the weights of that unit are multiplied by q.\n",
    "\n",
    "<img src=\"images/dropout1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inverted Dropout\n",
    "\n",
    "Inverted dropout is a variant of the original dropout technique developed <ahref='papers/JMLRdropout.pdf'>Srivastava et al.(2014)</a>. Just like the traditional dropout, inverted dropout randomly dropp out some fraction of the nodes.\n",
    "\n",
    "The one difference is that, during training of a neural network using inverted dropout the weights of the network are scaled-down  by the inverse of the keep probability (probability of units retained) q=1-p  and does not need any scaling during test time.\n",
    "\n",
    "In contrast, traditional dropout requires scaling during the test phase.\n",
    "<br>\n",
    "<br><img src=\"images/invtdrop.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/dropout1.jpg\"/>\n",
    "  (source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more on dropout read :\n",
    "<a href='https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf'>Dropout: A Simple Way to Prevent Neural Networks from Overfitting {Srivastava et al.\n",
    "(2014)}</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=torchvision.datasets.MNIST(root='./data/',train=True,transform=transforms.ToTensor())\n",
    "test_data=torchvision.datasets.MNIST(root='./data/',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hidden_1,num_hidden_2,lr,batch_size = 784, 10, 256, 256,0.5,256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter=DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "test_iter=DataLoader(dataset=test_data,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Inverted Dropout from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X,dropout_rate):\n",
    "    assert 0<=dropout_rate <=1\n",
    "    # probability of units to retain\n",
    "    keep_prob=1-dropout_rate\n",
    "    # In this case, all elements are dropped out\n",
    "    if dropout_rate==1:\n",
    "        return torch.zeros_like(X)\n",
    "    # # In this case, all elements are kept\n",
    "    if dropout_rate==0:\n",
    "        return X\n",
    "    mask=torch.empty(X.shape).uniform_(0,1) <keep_prob\n",
    "    return mask *X/keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing our dropout function on a few example with probabilities 0, 0.5, and 1, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
       "        [ 9., 10., 11., 12., 13., 14., 15., 16.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.range(1,16).reshape(2,8)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout all units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keep all units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
       "        [ 9., 10., 11., 12., 13., 14., 15., 16.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout 0.5 of the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0., 12., 14.,  0.],\n",
       "        [ 0., 20., 22.,  0., 26., 28., 30., 32.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  0.,  8., 10., 12., 14., 16.],\n",
       "        [ 0., 20., 22.,  0., 26.,  0.,  0., 32.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 0.5 dropout rate we can the see that the fraction of nodes dropout are random and Because the nodes drop out are chosen randomly on every pass, the representations in each layer can't depend on the exact values taken by nodes in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,is_training=True):\n",
    "        super().__init__()\n",
    "        self.training = is_training\n",
    "        self.lr1=torch.nn.Linear(num_inputs,num_hidden_1)\n",
    "        self.lr2=torch.nn.Linear(num_hidden_1,num_hidden_2)\n",
    "        self.lr3=torch.nn.Linear(num_hidden_2,num_outputs)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "    def forward(self,inputs):\n",
    "        inputs=inputs.reshape(-1,784)\n",
    "        h1=self.relu(self.lr1(inputs))\n",
    "        if self.training==True:\n",
    "            h1=dropout_layer(h1,dropout_rate=0.5)\n",
    "        h2=self.relu(self.lr2(h1))\n",
    "        if self.training==True:\n",
    "            h2=dropout_layer(h2,0.5)\n",
    "        output=self.lr3(h2)\n",
    "        return output\n",
    "        \n",
    "net=MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "opt=torch.optim.SGD(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net,data_iterator):\n",
    "    pred_correct = 0\n",
    "    for  data,label in data_iterator:\n",
    "        data=data.reshape(-1,784)\n",
    "        output=net(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        pred_correct += (pred==label).float().sum().item()\n",
    "        return 100*pred_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.890625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net,test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.218459,train acc 93.750000,test acc 92.578125\n",
      "epoch 1, loss 0.337482,train acc 95.703125,test acc 96.093750\n",
      "epoch 2, loss 0.153460,train acc 97.265625,test acc 96.875000\n",
      "epoch 3, loss 0.120417,train acc 97.265625,test acc 97.656250\n",
      "epoch 4, loss 0.145466,train acc 97.656250,test acc 96.875000\n",
      "epoch 5, loss 0.201399,train acc 97.265625,test acc 96.484375\n",
      "epoch 6, loss 0.158104,train acc 98.046875,test acc 98.046875\n",
      "epoch 7, loss 0.112679,train acc 97.656250,test acc 97.265625\n",
      "epoch 8, loss 0.104840,train acc 98.046875,test acc 98.046875\n",
      "epoch 9, loss 0.085689,train acc 97.656250,test acc 98.046875\n",
      "epoch 10, loss 0.131020,train acc 98.046875,test acc 98.828125\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs+1):\n",
    "    test_acc,train_acc=0,0\n",
    "    for X,y in train_iter:\n",
    "        net.train()\n",
    "        y_hat=net(X)\n",
    "        l=loss_fn(y_hat,y)\n",
    "        opt.zero_grad() \n",
    "        l.backward() \n",
    "        opt.step() \n",
    "    acc_te=evaluate_accuracy(net.eval(),test_iter)\n",
    "    net.eval()\n",
    "    acc_tr=evaluate_accuracy(net,train_iter)\n",
    "    test_acc+=acc_te\n",
    "    train_acc+=acc_tr\n",
    "    print('epoch %d, loss %f,train acc %f,test acc %f'%(epoch,l,train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCISE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=torch.nn.Sequential(torch.nn.Flatten(),\n",
    "         torch.nn.Linear(784,256),\n",
    "         torch.nn.Dropout(0.5),\n",
    "         torch.nn.ReLU(),\n",
    "         torch.nn.Linear(256,256),\n",
    "        torch.nn.Dropout(0.5),\n",
    "         torch.nn.ReLU(),\n",
    "         torch.nn.Linear(256,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Flatten()\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=256, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocgh 0, loss 2.294591\n",
      "epocgh 5, loss 2.297557\n",
      "epocgh 10, loss 2.314187\n",
      "epocgh 15, loss 2.303634\n",
      "epocgh 20, loss 2.308324\n",
      "epocgh 25, loss 2.299906\n",
      "epocgh 30, loss 2.296230\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs+1):\n",
    "    for X,y in train_iter:\n",
    "        #net.train()\n",
    "        y_hat=net(X)\n",
    "        l=loss_fn(y_hat,y)\n",
    "        opt.zero_grad() \n",
    "        l.backward() \n",
    "        opt.step() \n",
    "    if epoch%5==0:\n",
    "        print('epocgh %d, loss %f'%(epoch,l))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

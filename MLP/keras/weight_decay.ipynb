{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection, Underfitting and Overfitting\n",
    "\n",
    "-------\n",
    "\n",
    "The goal of machine learning is find a learning algorithm (algorithm that is able to learn from data) or simply model, train a model by adjusting the model parameters to get the best possible performance, both on the training (with minimum training error) and the test dat or new inputs (the trained model must be able to generalized well with minimum generalization error or test error) but the challenge in machine learning is how well does the trained model perform not just on the training data, but also on new unseen inputs (test inputs).This is a fundamental problem in machine learning between <b> optimization (the process of adjusting a model parameters to give the best possible performance on the training data) and generalization (how well does the trained model performs on newly unseen data) because a trained model can perform well well on the training dataset but performs poorly on newly unseen data points.</b>\n",
    "\n",
    "\n",
    "# NOTE\n",
    "------\n",
    "<h3 style='color:blue'>The training error is the error of our model as calculated on the training dataset</h3>\n",
    "<h3 style='color:blue'>The generalization error is the expected value of the error on a test or new data points drawn from the same underlying data distribution as our original sample</h3>\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "```\n",
    "The factors determining how well a machine learning algorithm will perform are its ability to:\n",
    "1. Make the training error small.\n",
    "2. Make the gap between training and test error small.\n",
    "These two factors correspond to the two central challenges in machine learning: underfitting and overfitting.\n",
    "\n",
    "(source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 111) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overftting\n",
    "\n",
    "-------\n",
    "When the complexity of the model is too high (highly flexible models) as compared to the underlying distribution of the data the model is trying to learn from, it tends to learn the noise present in data and is called overfitting. An Overfitted models has it training error much lower than validation error. <b>An overfitted model fails to Generalize well and has high Variance and Low Bias and the techniques used to combat overfitting are called regularization</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting\n",
    "----\n",
    "Underfitting occurs when the model can neither obtain sufficiently low error value on the training set nor generalize to new data and has low Variance and high Bias. Underfitted models are not able to reduce the training error. W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "----\n",
    "\n",
    "## Regularization are techniques used to combat overfitting  and this reduces the test error or generalization erro\n",
    "\n",
    "```\n",
    " Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error \n",
    " but not its training error\n",
    " \n",
    " (source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 120)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  WEIGHT  REGULARIZATION\n",
    "<img src='images/we.jpg'>\n",
    "(source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 120)\n",
    "<img src='images/weight.jpg'>\n",
    "(source: From the book, Deep Learning with python by Fran√ßois Chollet, page 107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Weight decay is also known as L2 regularization or ridge regression or Tikhonov regularization\n",
    "\n",
    "<b>L2 regularization is also called weight decay in the context of neural networks</b> prevent the weights from growing too large unless it is really necessary. It can be realized by\n",
    "adding a term to the cost OR objective function that penalizes large weights and is defined as\n",
    "\n",
    "$$\\tilde{\\ell}(w)=\\ell(w) + \\frac{\\lambda}{2}w^{2} $$\n",
    "\n",
    "where $ \\tilde{\\ell}$ is the regularized cost fucbtion $\\ell_{0}$ is an error measure (usually the sum of squared errors) and $\\lambda$ is a hyperparameter chosen ahead of time that controls how weights are penalized. (weights the relative contribution of the norm penalty term $w^{2} $  relative to the standard objective function $\\ell$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "with the corresponding parameter gradient\n",
    "$$\\bigtriangledown \\tilde{\\ell}_{w}(w)=\\bigtriangledown \\ell_{w}(w) + \\lambda w $$\n",
    "\n",
    "The new updated weight after an iteration can be expressed as\n",
    "$$w=w-\\eta \\bigtriangledown\\tilde{\\ell}_{w}(w)=w-\\eta(\\lambda w +\\bigtriangledown \\ell_{w}(w)) $$\n",
    "\n",
    "$$ w=(1- \\eta\\lambda) w -\\eta \\bigtriangledown \\ell_{w}(w)) $$\n",
    "where $\\eta$ is the learning rate\n",
    "\n",
    "\n",
    "The addition of the weight decay term has modified the learning rule of the weight vector by a constant factor on each step just before updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression, the objective function, sum of squared errors is defined as\n",
    "$$e=(Xw-y)^{T}(Xw-y)$$\n",
    "\n",
    "When L2 regularization is added, the objective function changes to\n",
    "$$e=(Xw-y)^{T}(Xw-y)+ \\frac{\\lambda}{2}w^{2}$$\n",
    "\n",
    "and this the solution $w$ from\n",
    "$$ w=(XX^{T})^{-1}X^{T}y $$\n",
    "\n",
    "$$ To$$\n",
    "\n",
    "$$ w=(XX^{T}   + \\lambda  I )^{-1}X^{T}y $$\n",
    "\n",
    "Where the diagonal entries of this matrix $ \\lambda  I $ correspond to the variance of each input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/lp.jpg'>\n",
    " (source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 155-156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the effects of weight regularization \n",
    "<a href='https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1947&rep=rep1&type=pdf'>A Simple Weight Decay Can Improve Generalization by Anders Krogh and John A. Hertz</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T22:57:55.990446Z",
     "start_time": "2020-09-07T22:57:55.947447Z"
    }
   },
   "source": [
    "# High-Dimensional Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:40.734046Z",
     "start_time": "2020-09-16T19:12:22.149717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import d2l\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/highp.jpg'>\n",
    "(source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:40.749641Z",
     "start_time": "2020-09-16T19:12:40.740578Z"
    }
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = tf.zeros((num_examples, w.shape[0]))\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    y=X@w+b\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:40.789637Z",
     "start_time": "2020-09-16T19:12:40.756641Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b=tf.zeros((num_inputs,1))*0.01,0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:40.813641Z",
     "start_time": "2020-09-16T19:12:40.796643Z"
    }
   },
   "outputs": [],
   "source": [
    "features, labels= synthetic_data(true_w, true_b, n_train)\n",
    "x_test,y_test = synthetic_data(true_w, true_b, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:40.833639Z",
     "start_time": "2020-09-16T19:12:40.818642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:40.851640Z",
     "start_time": "2020-09-16T19:12:40.840641Z"
    }
   },
   "outputs": [],
   "source": [
    "class L2(keras.regularizers.Regularizer):\n",
    "    def __init__(self,strength):\n",
    "        self.strength=strength\n",
    "    def call(self,w):\n",
    "        return self.strength *tf.reduce_sum(tf.square(w))\n",
    "    \n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def mean_absolute_error(y_true,y_pred):\n",
    "    absolute=keras.backend.abs(y_true-y_pred)\n",
    "    return keras.backend.mean(absolute,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:41.019299Z",
     "start_time": "2020-09-16T19:12:40.858642Z"
    }
   },
   "outputs": [],
   "source": [
    "net=keras.models.Sequential()\n",
    "net.add(keras.layers.Dense(10,activation='relu',input_shape=(200,),use_bias=True,bias_initializer='zeros',\n",
    "                           kernel_regularizer=L2(0.05)))\n",
    "net.add(keras.layers.Dense(5,activation='relu',kernel_regularizer=L2(0.01)))\n",
    "net.add(keras.layers.Dense(1))\n",
    "net.compile(optimizer='sgd',loss=mean_squared_error,metrics=[mean_absolute_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:41.035828Z",
     "start_time": "2020-09-16T19:12:41.026300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,071\n",
      "Trainable params: 2,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:12:42.595726Z",
     "start_time": "2020-09-16T19:12:41.047827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4483 - mean_absolute_error: 0.5065 - val_loss: 0.0468 - val_mean_absolute_error: 1.7843\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1108 - mean_absolute_error: 0.2697 - val_loss: 0.0437 - val_mean_absolute_error: 1.7342\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0515 - mean_absolute_error: 0.1840 - val_loss: 0.0425 - val_mean_absolute_error: 1.7132\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0287 - mean_absolute_error: 0.1331 - val_loss: 0.0418 - val_mean_absolute_error: 1.6992\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0189 - mean_absolute_error: 0.1012 - val_loss: 0.0415 - val_mean_absolute_error: 1.6886\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0140 - mean_absolute_error: 0.0808 - val_loss: 0.0412 - val_mean_absolute_error: 1.6814\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0111 - mean_absolute_error: 0.0681 - val_loss: 0.0410 - val_mean_absolute_error: 1.6755\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0091 - mean_absolute_error: 0.0589 - val_loss: 0.0409 - val_mean_absolute_error: 1.6703\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0077 - mean_absolute_error: 0.0519 - val_loss: 0.0408 - val_mean_absolute_error: 1.6659\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0066 - mean_absolute_error: 0.0464 - val_loss: 0.0407 - val_mean_absolute_error: 1.6619\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0057 - mean_absolute_error: 0.0422 - val_loss: 0.0406 - val_mean_absolute_error: 1.6588\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0388 - val_loss: 0.0405 - val_mean_absolute_error: 1.6559\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0044 - mean_absolute_error: 0.0358 - val_loss: 0.0405 - val_mean_absolute_error: 1.6529\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0035 - mean_absolute_error: 0.0322 - val_loss: 0.0404 - val_mean_absolute_error: 1.6504\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0030 - mean_absolute_error: 0.0291 - val_loss: 0.0404 - val_mean_absolute_error: 1.6482\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0027 - mean_absolute_error: 0.0266 - val_loss: 0.0403 - val_mean_absolute_error: 1.6465\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0025 - mean_absolute_error: 0.0246 - val_loss: 0.0403 - val_mean_absolute_error: 1.6446\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0024 - mean_absolute_error: 0.0230 - val_loss: 0.0402 - val_mean_absolute_error: 1.6428\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0023 - mean_absolute_error: 0.0218 - val_loss: 0.0401 - val_mean_absolute_error: 1.6410\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0022 - mean_absolute_error: 0.0208 - val_loss: 0.0401 - val_mean_absolute_error: 1.6392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1723cc0bfd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(features,labels,steps_per_epoch=5,validation_data=(x_test,y_test),validation_steps=5,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Test-Time\" data-toc-modified-id=\"Test-Time-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Test Time</a></span></li><li><span><a href=\"#Inverted-Dropout\" data-toc-modified-id=\"Inverted-Dropout-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Inverted Dropout</a></span></li><li><span><a href=\"#Implementing-Inverted-Dropout-from-Scratch\" data-toc-modified-id=\"Implementing-Inverted-Dropout-from-Scratch-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Implementing Inverted Dropout from Scratch</a></span></li><li><span><a href=\"#CONCISE-IMPLEMENTATION\" data-toc-modified-id=\"CONCISE-IMPLEMENTATION-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CONCISE IMPLEMENTATION</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "The term <b>\"dropout\"</b> refers to dropping out units in a neural network. It is a technique for addressing overfitting. It consists of randomly dropping out some fraction of the nodes (setting fraction of the units to zero (injecting noise)) in each layer before calculating subsequent layer during training and has become a standard technique for training neural networks. When dropout is applied, during training its zeros out some fraction of the nodes with probability p in each layer before calculating the subsequent layer and the resulting network can be viewed as a subset of the original network. Because the fraction of the nodes that are drop out are chosen randomly on every pass, the representations in each layer can't depend on the exact values taken by nodes in the previous layer. \n",
    "\n",
    "<b> Dropout rate</b> is the fraction of the nodes in a layer that are zeroed out and itâ€™s usually set between 0 and 1.\n",
    "\n",
    "  \n",
    " ## Test Time\n",
    "\n",
    "<b>Typically at test time we disable dropout.</b> Given a trained model and a new example, we do not drop out any nodes and thus do not need to normalize. \n",
    "In traditional dropout the weights of the network at test time are scaled versions of the trained weights. If a unit is retained with <b>probability q=1-p</b> during training,S at test time the weights of that unit are multiplied by q.\n",
    "\n",
    "<img src=\"../images/dropout1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inverted Dropout\n",
    "\n",
    "Inverted dropout is a variant of the original dropout technique developed <ahref='papers/JMLRdropout.pdf'>Srivastava et al.(2014)</a>. Just like the traditional dropout, inverted dropout randomly dropp out some fraction of the nodes.\n",
    "\n",
    "The one difference is that, during training of a neural network using inverted dropout the weights of the network are scaled-down  by the inverse of the keep probability (probability of units retained) q=1-p  and does not need any scaling during test time.\n",
    "\n",
    "In contrast, traditional dropout requires scaling during the test phase.\n",
    "<br>\n",
    "<br><img src=\"../images/invtdrop.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"../images/dropout1.jpg\"/>\n",
    "  (source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more on dropout read :\n",
    "<a href='papers/JMLRdropout.pdf'>Dropout: A Simple Way to Prevent Neural Networks from Overfitting {Srivastava et al.\n",
    "(2014)}</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models,layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000, 784).astype(\"float32\") / 255,\n",
    "                                              y_train.astype(\"float32\")))\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test.reshape(10000, 784).astype(\"float32\") / 255,\n",
    "                                         y_test.astype(\"float32\")))\n",
    "test_data = test_data.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Inverted Dropout from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X,dropout_rate):\n",
    "    assert 0<=dropout_rate <=1\n",
    "    # probability of units to retain\n",
    "    keep_prob=1-dropout_rate\n",
    "    # In this case, all elements are dropped out\n",
    "    if dropout_rate==1:\n",
    "        return tf.zeros_like(X)\n",
    "    # # In this case, all elements are kept\n",
    "    if dropout_rate==0:\n",
    "        return X\n",
    "    mask=tf.random.uniform(shape=tf.shape(X), minval=0, maxval=1) <keep_prob\n",
    "    return tf.cast(mask, dtype=tf.float32) * X/keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing our dropout function on a few example with probabilities 0, 0.4, 0.5, and 1, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
       "       [ 9., 10., 11., 12., 13., 14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(1,17,dtype=tf.float32)\n",
    "X=tf.reshape(X,(2, 8))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout all units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keep all units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
       "       [ 9., 10., 11., 12., 13., 14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout 0.5 of the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 0.,  4.,  6.,  0.,  0.,  0.,  0., 16.],\n",
       "       [18.,  0.,  0., 24.,  0.,  0.,  0., 32.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 2.,  0.,  6.,  0., 10., 12., 14., 16.],\n",
       "       [18., 20.,  0.,  0.,  0., 28., 30.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(X,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 0.5 dropout rate we can the see that the fraction of nodes dropout are random and Because the nodes drop out are chosen randomly on every pass, the representations in each layer can't depend on the exact values taken by nodes in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=20, input_dim=32,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = self.add_weight(shape=(input_dim, units), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hidden_1,num_hidden_2 = 784, 10, 50,20\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1=Linear(input_dim=num_inputs,units=num_hidden_1)\n",
    "        self.layer2=Linear(input_dim=num_hidden_1,units=num_hidden_2)\n",
    "        self.layer3=Linear(input_dim=num_hidden_2,units=num_outputs)\n",
    "    def call(self,inputs):\n",
    "        h_1=tf.maximum(self.layer1(inputs),0)\n",
    "        h_1=dropout_layer(h_1,dropout_rate=0.3)\n",
    "        h_2=tf.maximum(self.layer2(h_1),0)\n",
    "        h_2=dropout_layer(h_2,dropout_rate=0.5)\n",
    "        output=tf.math.softmax(self.layer3(h_2))\n",
    "        return output\n",
    "    \n",
    "mlp=MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    pred_correct = 0.\n",
    "    for i ,(data,label) in enumerate(data_iterator):\n",
    "        output=net(data)\n",
    "        output=output.numpy()\n",
    "        pred=np.argmax(output,axis=1)\n",
    "        pred_correct=np.sum(pred==label)\n",
    "        return (pred_correct/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a metric object\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.817480,train acc 0.664062,test acc 0.695312\n",
      "epoch 1, loss 1.743118,train acc 0.671875,test acc 0.718750\n",
      "epoch 2, loss 1.659132,train acc 0.773438,test acc 0.742188\n",
      "epoch 3, loss 1.652222,train acc 0.742188,test acc 0.710938\n",
      "epoch 4, loss 1.628205,train acc 0.761719,test acc 0.781250\n",
      "epoch 5, loss 1.577156,train acc 0.820312,test acc 0.796875\n",
      "epoch 6, loss 1.639755,train acc 0.847656,test acc 0.769531\n",
      "epoch 7, loss 1.619823,train acc 0.828125,test acc 0.859375\n",
      "epoch 8, loss 1.677131,train acc 0.832031,test acc 0.800781\n",
      "epoch 9, loss 1.597666,train acc 0.804688,test acc 0.835938\n",
      "epoch 10, loss 1.625037,train acc 0.847656,test acc 0.816406\n",
      "epoch 11, loss 1.613111,train acc 0.812500,test acc 0.835938\n",
      "epoch 12, loss 1.563551,train acc 0.773438,test acc 0.812500\n",
      "epoch 13, loss 1.505152,train acc 0.847656,test acc 0.843750\n",
      "epoch 14, loss 1.577855,train acc 0.808594,test acc 0.882812\n"
     ]
    }
   ],
   "source": [
    "epochs=15\n",
    "for e in range(epochs):\n",
    "    train_acc,test_acc=0,0\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(train_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = mlp(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "        # Update the state of the `accuracy` metric.\n",
    "        acc_c=accuracy.update_state(y, logits)\n",
    "        # Update the weights of the model to minimize the loss value.\n",
    "        gradients = tape.gradient(loss_value, mlp.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "    train_accuracy = evaluate_accuracy(train_data, mlp)\n",
    "    test_accuracy = evaluate_accuracy(test_data, mlp)\n",
    "    train_acc+=train_accuracy\n",
    "    test_acc+=test_accuracy\n",
    "    print('epoch %d, loss %f,train acc %f,test acc %f'%(e,loss_value,train_acc,test_acc))     \n",
    "    # Result the metric's state at the end of an epoch\n",
    "    accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCISE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images.reshape(60000, 784).astype(np.float32)/255\n",
    "test_images=test_images.reshape(10000, 784).astype(np.float32)/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(sequence,dim=10):\n",
    "    results=np.zeros((len(sequence),dim))\n",
    "    for i, seq in enumerate(sequence):\n",
    "        results[i,seq]=1# set the indices of one_hot[i] to 1s \n",
    "    return results\n",
    "\n",
    "train_labels=one_hot(train_labels)\n",
    "test_labels=one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_vdata,y_train,y_vdata=train_images[30000:], train_images[:30000], train_labels[30000:],train_labels[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizeDropout(layers.Layer):\n",
    "    def __init__(self,rate):\n",
    "        super().__init__()\n",
    "        self.rate=rate\n",
    "    def call(self,inputs,training=False):\n",
    "        if training:\n",
    "            return dropout_layer(X=inputs,dropout_rate=self.rate)\n",
    "            #returnself.rate*inputs\n",
    "        else:\n",
    "            return inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=models.Sequential()\n",
    "net.add(layers.Dense(100,activation='relu',input_shape=(784,)))\n",
    "net.add(customizeDropout(0.5))\n",
    "net.add(layers.Dense(50,activation='relu'))\n",
    "net.add(customizeDropout(0.4))\n",
    "net.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net=models.Sequential()\n",
    "net.add(layers.Dense(20,activation='relu',input_shape=(784,)))\n",
    "net.add(layers.Dropout(0.5))\n",
    "net.add(layers.Dense(15,activation='relu'))\n",
    "net.add(layers.Dropout(0.5))\n",
    "net.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "customize_dropout_1 (customi (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "customize_dropout_2 (customi (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_vdata,y_train,y_vdata=train_images[30000:], train_images[:30000], train_labels[30000:],train_labels[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 30000 samples\n",
      "Epoch 1/9\n",
      "30000/30000 [==============================] - 2s 76us/step - loss: 0.4306 - accuracy: 0.8795 - val_loss: 0.2616 - val_accuracy: 0.9250\n",
      "Epoch 2/9\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.2002 - accuracy: 0.9416 - val_loss: 0.1814 - val_accuracy: 0.9474\n",
      "Epoch 3/9\n",
      "30000/30000 [==============================] - 2s 65us/step - loss: 0.1457 - accuracy: 0.9573 - val_loss: 0.1458 - val_accuracy: 0.9572\n",
      "Epoch 4/9\n",
      "30000/30000 [==============================] - 2s 64us/step - loss: 0.1112 - accuracy: 0.9671 - val_loss: 0.1313 - val_accuracy: 0.9615\n",
      "Epoch 5/9\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.0887 - accuracy: 0.9724 - val_loss: 0.1215 - val_accuracy: 0.9646\n",
      "Epoch 6/9\n",
      "30000/30000 [==============================] - 2s 64us/step - loss: 0.0732 - accuracy: 0.9778 - val_loss: 0.1195 - val_accuracy: 0.9659\n",
      "Epoch 7/9\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.0590 - accuracy: 0.9823 - val_loss: 0.1135 - val_accuracy: 0.9665\n",
      "Epoch 8/9\n",
      "30000/30000 [==============================] - 2s 68us/step - loss: 0.0484 - accuracy: 0.9854 - val_loss: 0.1195 - val_accuracy: 0.9665\n",
      "Epoch 9/9\n",
      "30000/30000 [==============================] - 2s 65us/step - loss: 0.0397 - accuracy: 0.9879 - val_loss: 0.1137 - val_accuracy: 0.9682\n"
     ]
    }
   ],
   "source": [
    "results=net.fit(x_train,y_train,batch_size=100,validation_data=(x_vdata,y_vdata),epochs=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = net.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696000218391418\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
